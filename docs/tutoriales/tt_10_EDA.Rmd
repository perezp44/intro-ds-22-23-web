---
title: "EDA y modelos (datos de bebes)"
author: "Pedro J. Pérez (pedro.j.perez@uv.es). Universitat de València <br> <br> Web del curso: <https://perezp44.github.io/intro-ds-22-23-web/>"
date: "Septiembre de 2020 (actualizado el `r format(Sys.time(), '%d %B %Y')`)"
output:
  html_document:
    css: !expr here::here("assets", "bitacora.css")  
    theme: paper
    highlight: textmate
    self_contained: yes
    number_sections: no
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: true
      smooth_scroll: false
---

```{r chunk_setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE, 
                      cache = FALSE, cache.path = "/caches/", comment = "#>",
                      #fig.width = 7, fig.height= 7,   
                      #out.width = 7, out.height = 7, 
                      # fig.asp = 7/9, out.width = "60%",  (lo q yo usaba)
                      collapse = TRUE,  fig.show = "hold",
                      fig.asp = 0.618, fig.width = 9, out.width = "90%", fig.align = "center")
```

```{r, echo = FALSE, message = FALSE}
options(scipen = 999) #- quitar notación científica
options("yaml.eval.expr" = TRUE)  #- pa que no se queje el yaml de evaluar expr: https://github.com/viking/r-yaml/issues/47
```


```{r klippy, echo = FALSE}
klippy::klippy(position = c("top", "right")) #- remotes::install_github("rlesur/klippy")
```

```{r setup, echo = FALSE}
library(knitr)
library(here)
library(tidyverse)
library(patchwork)
library(pjpv2020.01)
```

-------------

<br>

# 1. Introducción

Hemos visto en tutoriales anteriores como cargar, manejar, arreglar y visualizar datos à la tidyverse. Una vez somos capaces de manejar y arreglar nuestros datos con R podemos pasar a efectuar un análisis "realista" con un conjunto de datos. 


En este tutorial y siguientes, vamos a utilizar un conjunto de datos sobre nacimientos de bebes en España para mostrar algunas técnicas de EDA y modelización. No se mostrarán los detalles técnicos de las métodos, sino solo su aplicación práctica y, a veces, la interpretación de los resultados.


Antes de proceder a la estimación de modelos estadísticos formales o contrastar hipótesis se suele realizar una etapa conocida como **análisis exploratorio**. El análisis exploratorio (EDA) es una parte importante de todo análisis de datos. Suele tener lugar antes de la especificación y estimación de modelos estadísticos formales. Su objetivo último es "comprender" los datos y las relaciones existentes entre las variables.

Una de las máximas de un data scientist es, como señalan [aquí](https://medium.com/@Randy_Au/data-science-foundations-know-your-data-really-really-know-it-a6bb97eb991c?source=friends_link&sk=42f1c02883e744df7dbb618373312244), "**Know Your Data**". Además nos explican que:


> Data exploration is the art of looking at your data, rapidly generating hypotheses, quickly testing them, then repeating again and again and again. The goal of data exploration is to generate many promising leads that you can later explore in more depth.

> With  exploratory data analysis, you’ll combine visualisation and transformation with your curiosity and scepticism to ask and answer interesting questions about data

Un buen ejemplo de qué es un análisis exploratorio son los *screencast* de David Robinson: cada semana, David graba un vídeo en el que efectúa un análisis rápido de un conjunto de datos. Puedes verlos, en su canal de youtube ,[aquí](https://www.youtube.com/user/safe4democracy/videos). En [este repo](https://github.com/dgrtwo/data-screencasts) de Github tienes la transcripción, en ficheros `.Rmd`, del código que acaba utilizando en los screencasts. Hay bastante gente que sigue sus vídeos y , como explican en [este post](https://paulvanderlaken.com/2020/06/16/david-robinsons-r-programming-screencasts/), varias personas mantienen un listado en el que diseccionan las tareas que hace David en cada screencast, puedes encontrarlos [aquí](https://docs.google.com/spreadsheets/d/1pjj_G9ncJZPGTYPkR1BYwzA6bhJoeTfY2fJeGKSbOKM/edit#gid=444382177).


Por último, un buen libro, bueno un *bookdown*, sobre EDA: [Exploratory Data Analysis with R](https://bookdown.org/rdpeng/exdata/) de Roger Peng.



## ¿Qué queremos saber?

Generalmente un estudio cuantitativo comienza por una pregunta(s) que guía el análisis. En nuestro caso, el objetivo del tutorial es meramente mostrar algunas funciones y técnicas útiles en la fase de exploración de datos (EDA); aún así, durante el tutorial, nuestro "estudio" estará guiado por varias preguntas relacionadas con los bebes; por ejemplo: 1) vamos a intentar ver/contestar si los bebitos nacen con más peso que las bebitas, y 2) intentaremos contestar a la pregunta de si determinadas características de las madres/padres, como la edad o el nivel educativo, afectan al peso de los recién nacidos.

El siguiente paso de todo análisis empírico es la búsqueda de un conjunto de datos que permita (hasta cierto grado de confianza) dar una respuesta a las preguntas planteadas. ¿De donde sacamos esos datos? En nuestro caso, utilizaremos la estadística de nacimientos del INE. Los detalles en la sección siguiente.


-------------

<br>

# 2. Los datos

## Origen de los datos

Los datos provienen del **INE**, concretamente de la [**Estadística de nacimientos**](http://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736177007&menu=resultados&secc=1254736195443&idp=1254735573002). Como puedes ver en la web del INE hay una serie de tablas que muestran resultados para las principales variables; por ejemplo, hay una tabla llamada ["Nacimientos por edad de la madre, mes y sexo"](https://www.ine.es/jaxi/Tabla.htm?path=/t20/e301/provi/l0/&file=01001.px&L=0). Estas tablas están muy bien para mostrar los principales resultados de la Estadística de nacimientos, pero nosotros queremos analizar "de verdad" los datos, así que tendremos que descargarnos los [**microdatos** de la encuesta](https://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736177007&menu=resultados&secc=1254736195443&idp=1254735573002#!tabs-1254736195443).

En los microdatos de la Estadística de Nacimientos hay datos sobre: (i) Nacimientos, (ii) Muertes fetales tardías y (iii) Partos. Vamos a usar los ficheros de datos sobre **PARTOS**. Hay un fichero para cada año desde 1975, pero como han habido diversos cambios en la metodología de la estadística, solo vamos a trabajar los ficheros de los años **2007 a 2019** que comparten diccionario. 

Me descargue (a mano) los ficheros de microdatos de nacimientos para los años 2007 a 2018. Los datos están en formato texto, y cada registro es una cadena larga de caracteres. Los datos van acompañados de un diccionario que sirve para poder separar las cadenas de caracteres en los valores de cada variable. Os ahorro los detalles del proceso. 

Los datos eran de partos, pero los preparé para que cada fila pertenezca a los datos de un solo bebe: **cada bebe tiene su propia fila**.

Mi ordenador tuvo problemas para mover el fichero con todos los datos (2007 a 2018) y, como además, el objetivo del tutorial es tan sólo mostrar algunas técnicas y funciones útiles para EDA; por lo tanto, solamente utilizaremos datos referentes a 2016 y 2019 para bebes nacidos en la Comunidad Valenciana.


<!-- Detalles: Los datos los saco de los microdatos de partos del INE y los proceso en el Rproject `datos_bebes_2021`. Generalmente lo que hago es cargar unos datos completos con el fichero `01_coger-un-trozo-de-los-datos.R` y en ese script me quedo con un trozo de los datos que son los que uso para jugar. Los datos con los que voy a jugar están en la carpeta `./datos/usar`. -->

<br>

## Cargando el fichero de datos y dicc

Ya dijimos que los datos provienen del **INE**, concretamente de la [**Estadística de nacimientos**](http://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=1254736177007&menu=resultados&secc=1254736195443&idp=1254735573002). He restringido la muestra a los partos en la Comunitat Valenciana para los años 2016 y 2019.


```{r echo = TRUE}
#- Cargo datos q voy a usar y el diccionario. 
mys_archivos <- fs::dir_ls(here::here("datos", "usar"))         #- AQUI
dicc_df_all <- readxl::read_excel(mys_archivos[[2]])
df_all <-  readr::read_rds(mys_archivos[[1]])
```


```{r echo = FALSE}
#- Cargo datos, diccionario. Hago 3 chequeos para comprobar que df y dicc_df vienen de los mismos datos
testthat::expect_equal(nrow(dicc_df_all), length(df_all))
testthat::expect_equal(names(df_all), dicc_df_all[[1]])     #- chequeo el nombre de las variables
testthat::expect_equal(nrow(df_all), dicc_df_all$NN_ok[1])  #- chequeo las filas 
```


```{r, echo = FALSE}
zzz <- glue::glue_collapse(unique(df_all$year_parto), sep = ", ", last = " y ")
```

Los datos tienen: **`r ncol(df_all)` variables y `r nrow(df_all)`** registros de bebes. Tenemos datos para los años: **`r zzz`**. Hay muchas variables, iremos seleccionado las variables que nos interesan para cada visulización

```{r, echo = FALSE}
no_quitar <- c("df_all", "dicc_df_all")
rm(list=ls()[! ls() %in% no_quitar])
```

```{r, echo = FALSE}
no_quitar <- c("df_all", "dicc_df_all")
rm(list=ls()[! ls() %in% no_quitar])
```



Hay muchas variables, concretamente `r ncol(df)` variables; así que en algún momento tendremos que centrarnos en algunas de ellas.

## Diccionario

Lo primero es conocer tus variables, para ello tiene que haber un diccionario y lo hay, pero en nuestro caso el nombre de las variables (casí) es suficiente.


```{r}
names(df_all)
```

<br>

## 2 tablas

-  Hago **2 tablas** para poder buscar las variables de forma rápida y ver sus principales características, aunque lo mejor es usar `dicc_df_all` ya sea en el Global o el diccionario original en local:

```{r, echo = TRUE}
dicc_show_1 <- dicc_df_all %>% 
  select(variable, type, nn_unique, unique_values, p_na, p_zeros) %>% 
  mutate(unique_values = stringr::str_sub(unique_values, 1, 45))
```

- 1. Tabla con `DT`


```{r, echo = TRUE}
DT::datatable(dicc_show_1, filter = 'top', extensions = "Scroller", 
              options = list(autoWidth = TRUE,deferRender = TRUE, 
                             scroller = TRUE, scrollY = 450 ))
```

<br>

- 2. Tabla con `reactable`

```{r, eval = TRUE, echo = TRUE}
# Tabla con el paquete [`reactable`](https://glin.github.io/reactable/index.html)
dicc_show_2 <- dicc_df_all %>% 
  select(variable, type, nn_unique, unique_values, p_na, p_zeros) 
   
library(reactable)
reactable::reactable(dicc_show_2, pagination = FALSE, height = 450, filterable = TRUE, searchable = TRUE, highlight = TRUE, columns = list( variable = colDef(
     # sticky = "left",
      # Add a right border style to visually distinguish the sticky column
      style = list(borderRight = "1px solid #eee"),
      headerStyle = list(borderRight = "1px solid #eee")
    )),
  defaultColDef = colDef(minWidth = 70)
)                     
```


```{r, echo = FALSE}
#- arreglo algunas variables (sexo_bebe: pasarlo a factor)
#- xq se me olvido en el script original
df_all <- df_all %>% mutate(sexo_bebe.f = as_factor(case_when(
  sexo_bebe == 1 ~ "bebito",
  sexo_bebe == 6 ~ "bebita",
  TRUE ~ NA_character_
)))
```



<br>

--------------------------------------

<br>


## Datos simplificados

Para que todo vaya más fluido vamos a utilizar un fichero de datos reducido:


```{r, echo = FALSE}
#- se me paso pasarlo a factor
df_all <- df_all %>% 
  mutate(sexo_bebe.f = as_factor(case_when(
         sexo_bebe == 1 ~ "bebito",
         sexo_bebe == 6 ~ "bebita",
         TRUE ~ NA_character_  )))
```



```{r, echo = FALSE, eval = FALSE}
my_vv_1 <- c("prov_inscrip", "prov_inscrip.n", "muni_inscrip", "muni_inscrip.n", "year_parto", "fecha_parto", "lugar_parto.f", "parto_asistido.f", "parto_cesarea.f", "parto_nn_semanas",      
  "nn_nacidos_parto_con_o_sin_vida", "pais_nacionalidad_madre.n", "pais_nacionalidad_padre.n", 
  "pais_naci_madre.n", "pais_naci_padre.n", 
  "edad_madre.1", "edad_padre.1",
  "estudios_madre.ff", "ocupacion_madre.f", "estudios_padre.ff", "ocupacion_padre.f",
   "nn_hijos_tot", "nn_hijos_vivos_anteriores",
  "intervalo_parto_pareja_madre.1",
  "intervalo_intergenesico", "intervalo_parto_anterior.1", "intervalo_parto_anterior.2",
  "sexo_bebe", "peso_bebe", "bebe_muerto_cuando.f" , "bebe_muerto.f" )

df_1 <- df_all %>% select(all_of(my_vv_1))
```


```{r, echo = FALSE}
my_vv_2 <- c("fecha_parto", "prov_inscrip.n", "tamanyo_muni_inscri.f", 
             "parto_cesarea.f", "parto_normal.f", "parto_nn_semanas", 
             "sexo_bebe.f", "peso_bebe", 
             "pais_nacionalidad_madre.n", "edad_madre.1", "estudios_madre.ff", 
             "pais_nacionalidad_padre.n", "edad_padre.1", "estudios_padre.ff", 
             "nn_hijos_tot")

df_2 <- df_all %>% select(all_of(my_vv_2))
```




Al final, las variables que usaremos son:

```{r}
#- Al final usaremos estas variables
df <- df_2
names(df)
```

Vamos a tener **`r ncol(df)` variables** y **`r nrow(df)` bebes**


Hagamos un diccionario para la base de datos reducida que al final usaremos:

```{r}
#- creo dicc. de los daros q voy a USAR 
df_aa <- pjpv2020.01::pjp_f_estadisticos_basicos(df)
df_bb <- pjpv2020.01::pjp_f_unique_values(df, truncate = TRUE, nn_truncate = 50)
dicc_df <- bind_cols(df_aa, df_bb)
dicc_df <- dicc_df %>% select(variable, type, nn_unique, unique_values, q_na, p_na, p_zeros, min, max, mean, sd, NN, NN_ok)
```


```{r}
DT::datatable(dicc_df, filter = 'top', extensions = "Scroller", options = list(
    autoWidth = TRUE, 
    deferRender = TRUE,
    scroller = TRUE,
    scrollY = 500
))
```


```{r, echo = FALSE}
no_quitar <- c("df_all", "dicc_df_all", "df_2", "df", "dicc_df", "my_vv_2")
rm(list=ls()[! ls() %in% no_quitar])
```


<br>

-----------

<br>

# 3. Cosas de EDA


Hemos visto en tutoriales anteriores como cargar, arreglar y manejar datos con R *à la tidyverse*. En este tutorial vamos a ver **algunos paquetes y funciones que conviene conocer porque nos pueden ayudar a acelerar el análisis exploratorio inicial de nuestros datos**.


```{r, echo = FALSE, out.width = "90%", fig.align = "default"}
knitr::include_graphics(here::here("imagenes", "img_tt_10-01.png"))
```


En este [repo de Github](https://github.com/mstaniak/autoEDA-resources), Mateusz Staniak mantiene un listado de paquetes y recursos relacionados con la EDA.

En [este post](http://smarterpoland.pl/index.php/2019/04/explore-the-landscape-of-r-packages-for-automated-data-exploration/) y en [este artículo](https://arxiv.org/pdf/1904.02101.pdf) nos hablan del paquete [`autoEDA`](https://github.com/XanderHorn/autoEDA) que trata de automatizar el proceso inicial exploratorio de datos (EDA). Además el post nos muestra los 11 paquetes de R, relacionados con la EDA, más descargados de CRAN. El objetivo de estos 11/12 paquetes es similar, tratar de automatizar/facilitar el análisis preliminar o exploratorio de los datos (EDA). Evidentemente automatizar totalmente la EDA es muy-muy complicado, por no decir imposible, pero al menos estos paquetes (y otros muchos) contienen funciones que nos pueden ayudar en las primeras etapas de nuestros análisis de datos.

Por ejemplo, en [este post](https://www.littlemissdata.com/blog/simple-eda), Laura Ellis, nos explica como hacer EDA en R mostrándonos algunas de sus funciones/trucos favoritos como la función `skimr::skim()`, `visdat::vis_dat()` o `DataExplorer::create_report()`. El post tuvo una segunda parte ([aquí](https://www.littlemissdata.com/blog/inspectdf)), donde Laura nos explica el uso de algunas funciones útiles para EDA del paquete [`inspectdf`](https://github.com/alastairrushworth/inspectdf) como por ejemplo: `inspectdf::inspect_types()`, `inspectdf::inspect_na`, etc...

En este [otro post](https://sharla.party/posts/new-data-strategies/), Sharla Gelfand nos habla sobre estrategias para afrontar la EDA cuando se trabaja con un conjunto de datos nuevo. Sharla utiliza, entre otros, los paquetes [`visdat`](http://visdat.njtierney.com/), [`skimr`](https://ropensci.github.io/skimr/) y [`assertr`](https://docs.ropensci.org/assertr/).

Un [post más reciente](https://yuzar-blog.netlify.app/posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress/), de 2021, nos habla de 3 paquetes que hacen informes exploratorios completos de los datos: [DataExplorer](http://boxuancui.github.io/DataExplorer/), [SmartEDA](https://daya6489.github.io/SmartEDA/) y [dlookr](https://choonghyunryu.github.io/dlookr/).


De entre los muchos paquetes y funciones relacionados con EDA, conviene conocer, en mi opinión, estos^[Seguro que se me olvidan muchos; además el ecosistema R está en constante evolución, así que seguro que surgen mejoras]:


<br>

## 3.0 Informes completos

Hay varios paquetes que permiten realizar informes completos de un dataset de forma completamente automatizada. Por ejemplo:


- Con el paquete [DataExplorer](http://boxuancui.github.io/DataExplorer/)

```{r, eval = FALSE}
# informe sin especificar variable objetivo
DataExplorer::create_report(df)

# informe con  variable objetivo
create_report(df, y = "peso_bebe")
create_report(df, y = "sexo_bebe.f")
```

<br>

- Con el paquete [dlookr](https://choonghyunryu.github.io/dlookr/)


```{r, eval = FALSE}
dlookr::diagnose_report(df) 

# informe con variable objetivo
dlookr::eda_report(df, target = peso_bebe, 
                   output_format = "html", output_file = "EDA_bebes.html")

# ejemplo con NA's
dlookr::transformation_report(df, target = peso_bebe)

# ejemplo con  outliers
transformation_report(df, target = peso_bebe)
```

<br>

- Con el paquete [SmartEDA](https://daya6489.github.io/SmartEDA/)


```{r, eval = FALSE}
SmartEDA::ExpReport(df, op_file = 'smarteda.html')

# informe con variable objetivo
SmartEDA::ExpReport(df, op_file = 'smarteda.html', Target = "peso_bebe")
```

<br>

- Con el paquete [summarytools](https://github.com/dcomtois/summarytools)

```{r, eval = FALSE}
summarytools::dfSummary(df)
```

## 3.1 Nombres de las variables

Podemos cambiar y ver el nombre de las variables de varias maneras:

- Con `dplyr::rename()`

```{r, eval = FALSE}
df_copy <- df %>% dplyr::rename(peso_bebe_nuevo = peso_bebe) 
names(df_copy)[1] <- paste0(names(df_copy)[1], "_new") #- cambio el primer nombre
```

<br>

- Con `base::names()`

```{r, eval = FALSE}
names(df_copy)[1] <- paste0(names(df_copy)[1], "_new") #- cambio el primer nombre
names(df_copy)[2] <- "nuevo_nombre"    #- cambio el nombre de la segunda variable
```

- la función `janitor::clean_names()` arregla automáticamente los nombres de las variables, lo que pasa es que es nuestro ejemplo los nombres ya están suficientemente arreglados, pero si tuviésemos un data.frame con nombres extraños, muy largos o incluso con nombres "no sintácticos", y necesitaramos arreglarlos de forma rápida, podemos hacerlo con la función `janitor::clean_names()`, que arregla los nombres de las variables de forma automática, y además tiene algunas opciones para hacerlo como más te guste.

<br>

```{r, eval = FALSE}
janitor::clean_names(df) 
```



<br>

------------------------------------------------------------------------

<br>

## 3.2 Estructura y tipo de variables

Siempre-siempre hay que saber de que tipo son nuestras variables. Esto se puede hacer de muchas formas. Os muestro dos:

### a) con `str()`

```{r}
str(df)            #- str() muestra la estructura interna de un objeto R
```

<br>

### b) con `inspectdf::inspect_types()`

```{r}
inspectdf::inspect_types(df)   #- muestra de que tipo son las variables
```

<br>

----------------

<br>

## 3.3 Valores únicos de las variables

Conviene saber qué valores (únicos) tienen las distintas variables de una tabla de datos; por ejemplo, es obvio, pero hay que saber cual es el periodo muestral de nuestros datos y el hecho de que nuestros datos los trabajemos habitualmente en formato largo, hace que saber esto no sea tan-tan sencillo


- `pjpv2020.01::pjp_f_unique_values()` devuelve un df con los valores únicos de cada variable:


```{r}
zz <- pjpv2020.01::pjp_f_unique_values(df, truncate = TRUE, nn_truncate = 47) 
gt::gt(zz)
```

<br>

- `pjpv2020.01::pjp_f_unique_values()`. Parecida a la función anterior, pero devuelve los valores únicos de cada variable en una columna: a veces esto es mejor para poder visualizarlos


```{r, eval = FALSE}
zz <- pjpv2020.01::pjp_f_valores_unicos(df) 
gt::gt(zz)
```

<br>

- La función `dplyr::distinct()` es muy útil para ver los con valores únicos de una o varias variables. Por ejemplo:


```{r}
names(df)
zz <- df %>% distinct(prov_inscrip.n, parto_cesarea.f)
zz %>% gt::gt()
```



<br>

----------------

<br>

## 3.4 Estadísticos básicos de las variables

### a) con `pjpv2020.01::pjp_f_estadisticos_basicos()`

La función `pjpv2020.01::pjp_f_estadisticos_basicos()`. Sí, el nombre de la función es un poco largo, pero es que es un paquete para uso personal. Yo estoy acostumbrado a usarla. Da un resumen rápido de las variables del data.frame:

```{r, eval = FALSE}
zz <- pjpv2020.01::pjp_f_estadisticos_basicos(df)   #- estadísticos básicos del df
zz <- pjpv2020.01::pjp_f_unique_values(df)          #- valores únicos de cada variable de df
```

<br>

### b) con `summarytools::dfSummary()`

La función `summarytools::dfSummary()` da un informe/resumen de las variables de un data.frame muy útil. Es muy útil para hacerse una idea rápida de las propiedades de las variables, pero tiene la pega de que el output es muy voluminosos para mostrarlo aquí, así que si quieres verlo tendrás que ejecutar el anterior chunk de forma interactiva.

```{r, eval = FALSE}
zz <- summarytools::dfSummary(df) #- genera un fichero con un resumen útil y agradable de ver 
summarytools::view(zz)  #- para visualizar el informe
```

<br>

### c) con `skimr::skim()`

Otra alternativa es usar `skimr::skim(df). Ocurre lo mismo: como su output es voluminoso, no lo voy a mostrar aquí.

```{r, eval = FALSE}
skimr::skim(df)
```


### d) con `gtsummary::tbl_summary()`

```{r, eval = FALSE}
gtsummary::tbl_summary(df)
```

Además esta función permite especificar variables de agrupación y realizar algunos contrastes. Para ver todas las posibilidades que ofrece, la vignette está [aquí](https://www.danieldsjoberg.com/gtsummary/articles/tbl_summary.html)


```{r, eval = FALSE}
df %>% select(peso_bebe, parto_nn_semanas, estudios_madre.f, parto_cesarea.f) %>% 
  gtsummary::tbl_summary(by = "parto_cesarea.f") %>% 
  gtsummary::add_p()
names(df)
```



<br>

---------------

<br>


## 3.5 NA's

Siempre hay que chequear la existencia de **NA's** en nuestro df. NA representa una característica que existe pero no sabemos su valor En R, los valores no disponibles se representan con `NA`. Se puede chequear si un valor es NA con la función `is.na()`. En datos generados por otros paquetes estadísticos, los valores ausentes pueden estar codificados con diferentes valores como por ejemplo "-999", "N/A", un espacio en blanco, etc ...

Unas [slides](https://docs.google.com/presentation/d/1EzMU6be01aicPGeBcbXrXy1iX8CdWhBz1o2pJDPsXBM/edit?usp=sharing) de `@allison_horst` que ayudan a entender los distintos tipos de NA's. Además [aquí](https://ucsb.box.com/s/pr2z7ygeycrfwefmsdknrq3lurzka5u) está el video y un tutorial interactivo [aquí](https://allisonhorst.shinyapps.io/missingexplorer/)

Cuatro paquetes que nos pueden ayudar en esta tarea son `DataExplorer`, `visdat` , `naniar` y `dlookr`.

```{r}
DataExplorer::plot_missing(df)
```

```{r}
naniar::gg_miss_var(df, show_pct = TRUE)        
```

El paquete `visdat` es útil para ver los NA's pero el siguiente chunk no nos va a funcionar para las `r format(nrow(df), big.mark = "." )` filas que tiene nuestro df.

```{r, eval = FALSE}
visdat::vis_dat(df) #- no funciona , hay demasiadas filas
```

Pero veamos  como funcionaría para, por ejemplo, las primeras 1.000 filas de df:

```{r}
df %>% slice(1:1000) %>% visdat::vis_dat()
```

- Algo parecido pero con `visdat::vis_miss()`

```{r}
df %>% slice(1:1000) %>% visdat::vis_miss(cluster = TRUE) 
```

- Otra forma de visualizar la **co-ocurrencia de NA's** en las variables de df:


```{r}
naniar::gg_miss_upset(df)
```

- `dlookr::plot_na_intersect()` proporciona un gráfico muy útil. Nos ofrece las variables que tienen NA's y la co-ocurrencia de NA's entre ellas.

```{r, eval = FALSE}
dlookr::plot_na_intersect(df)
```

<br>

### Trabajando con los `NA's`

- Por ejemplo, podemos querer seleccionar las variables que tienen NA's:

```{r}
zz_con_NAs  <- df %>% select(where(anyNA))
```

<br>

- A lo mejor, nos puede interesar ver la ocurrencia de NA's para los grupos/categorías de una variable categórica de df, por ejemplo los estudios de la madre(`estudios_madre.f`)


```{r}
naniar::gg_miss_var(df, facet = estudios_madre.ff, show_pct = TRUE) #- faceted por la variable estudios_madre.ff
```



```{r, echo = FALSE}
rm(list = ls(pattern = "^zz"))  #- borra los objetos cuyo nombre empiece por zz
```


### ¿Que hacemos con los NA's?

Pues no está claro, depende ... pero supongamos que queremos dejar nuestro df **SIN NA's;** 

```{r}
zz <- df %>% tidyr::drop_na()              #- con tidyr

#- otras formas de hacer lo mismo ---
zz <- df[complete.cases(df), ]             #- con base-R 
zz <- df %>% filter(complete.cases(.))     #- con dplyr y base-R
```


Si quitáramos todos los registros/bebes que tienen algún NA en alguna de las `r ncol(df)` variables, nos quedaríamos con `r format(nrow(zz), big.mark = ".")` registros.

<br>

Se puede refinar la eliminación de `NA's`. Puedes ser que igual Igual nos interesase eliminar sólo las filas que tengan un `NA` en una determinada variable, por ejemplo, si quisieramos quitar las filas que tuviesen NA en la variable `peso_bebe` y/o en la variable `parto_nn_semanas`, lo haríamos así:

```{r}
zz <- df %>% tidyr::drop_na(c(peso_bebe, parto_nn_semanas))   #- quito filas con NA en peso_bebe o en SEMANAS
```

<br>

- Imagina que hubiese una fila o una columna con todos sus valores NA. Lógicamente habría que eliminarlas. Se puede hacer fácil con `janitor::remove_empty()

```{r, eval = FALSE}
zz <- df %>% janitor::remove_empty(c("rows", "cols"))    #- quita variables y filas vacías
```


<br>

Si por el contrario decides que lo adecuado es imputar valores a los NA's, puedes utilizar el paquete [mice](https://datascienceplus.com/handling-missing-data-with-mice-package-a-simple-approach/),  [amelia II](https://gking.harvard.edu/amelia) o `dlookr::imputate_na()`. Si decides tratar de predecir los valores de los NA's, [aquí](http://r-statistics.co/Missing-Value-Treatment-With-R.html) tienes un post. Otra opción es usar 


```{r, echo = FALSE}
rm(list = ls(pattern = "^zz"))  #- borra los objetos cuyo nombre empiece por zz
```

<br>

---------------

<br>


## 3.6 Variables numéricas

Vamos a ver algunas funciones útiles para hacer un análisis inicial de  las variables numéricas.

Si quisiéramos crear un df  sólo con las variables numéricas:


```{r}
df_numeric <- df %>% select_if(is.numeric)      #- antigua sintaxis
df_numeric <- df %>% select(where(is.numeric))  #- nueva API
```


<br>

### a) Estadísticos descriptivos

Por ejemplo, `summarytools::descr()` nos ayuda a calcular rápidamente estadísticos descriptivos de las variables numéricas.

```{r, results = "asis", eval = FALSE}
summarytools::descr(df, style = "rmarkdown")
```


```{r}
summarytools::descr(df)
```


Hay muchas otras alternativas, por ejemplo: `dlookr::describe()`, `dlookr::univar_numeric()`, `dlookr::diagnose_numeric()`,  `SmartEDA::ExpNumStat()`, `summarytools::



<br>

### b) Histogramas o f. de densidad

En general, los métodos estadísticos requieren que las variables sigan una determinada distribución. Esto puede chequearse formalmente o utilizar histogramas y/o funciones de densidad estimadas.


Podemos usar el paquete `DataExplorer`  para obtener histogramas o gráficos de densidad para las variables numéricas.

- Histogramas:


```{r}
DataExplorer::plot_histogram(df, ncol = 2)
```

- Funciones de densidad estimadas:

```{r}
DataExplorer::plot_density(df, ncol = 2)
```

<br>

### c) Matriz de correlaciones

Otro aspecto importante de un análisis exploratorio consiste en analizar la existencia de relaciones entre las variables del conjunto de datos. Esto puede hacerse de diversas maneras, dos de las más sencillas y usadas son los gráficos de dispersión y las matrices de correlación.

Podemos obtener la matriz de correlaciones de diversas formas:


- Primero con la función `cor()` de R-base:

```{r}
stats::cor(df_numeric, use = "pairwise.complete.obs") %>%  #- devuelve una matriz, no un df
      round(. , 2)
```



```{r}
#df_numeric %>% GGally::ggcorr(label = TRUE)
df_numeric %>% GGally::ggpairs()
```


- Ahora con la función `corrr::correlate()`

```{r}
df %>% select_if(is.numeric) %>% 
       corrr::correlate() %>% 
       pjpv2020.01::pjp_f_decimales(nn = 2) %>%  
       gt::gt()
```


<br>

- Otra forma más de visualizar las correlaciones, con `inspectdf::inspect_cor()`

```{r}
df %>% inspectdf::inspect_cor()
```

- Además, después, con `inspectdf::show_plot()` se pueden visualizar las correlaciones en un gráfico:

```{r}
df %>% inspectdf::inspect_cor() %>% inspectdf::show_plot()
```

<br>


- El paquete [correlation](https://easystats.github.io/correlation/) facilita el cálculo de diferentes estadísticos de correlación, por defecto calcula el coeficiente de correlación de Pearson:

```{r}
correlation::correlation(df_numeric)    #- remotes::install_github("easystats/correlation")
```

<br>

### d) Boxplots frente a una v. categórica

Para ver diferencias en la distribución de las variables numéricas frente a una variable categórica es muy útil `DataExplorer::plot_boxplot()`. Por ejemplo frente a la variable `estudios_madre.ff`: 


```{r}
DataExplorer::plot_boxplot(df, by = "estudios_madre.ff")
```

<br>

O, por ejemplo, frente a la variable `parto_cesarea.f`

```{r}
my_vv <- names(df)[3]
my_vv <- "parto_cesarea.f"
DataExplorer::plot_boxplot(df, by = my_vv)
```

<br>

- También se pueden hacer boxplots de una variables numérica frente a 2 categóricas:

```{r}
df %>% explore::explore(edad_madre.1, estudios_madre.ff, target = parto_cesarea.f)
```

<br>

- Con `explore::explore_all()` se muestran gráficos de todas las variables del df frente a una variable target u objetivo:

```{r}
df %>% select(sexo_bebe.f, edad_madre.1, peso_bebe,  estudios_madre.ff, parto_cesarea.f) %>% explore::explore_all(target = parto_cesarea.f)
```

Veamos la relación entre "estudios_madre.ff" y CESAREA.f:

```{r}
df %>% janitor::tabyl(estudios_madre.ff, parto_cesarea.f) %>% janitor::adorn_percentages()
```


<br>


### e) Scatterplots entre variables numéricas


`DataExplorer::plot_scatterplot()` nos permite hacer rápidamente un scatterplot de todas las variables del df frente a una variable, por ejemplo el peso del bebe: `peso_bebe`


```{r}
my_vv <- "peso_bebe"
DataExplorer::plot_scatterplot(df, by = my_vv, sampled_rows = 500L)
```

----------------

<br>




## 3.7 Variables categóricas


Si necesitásemos un df con todas las variables categóricas, ¿cómo lo obtendrías?


```{r, eval = FALSE}
df %>% select_if(is.factor) %>% names()      #- old fashion
df %>% select(where(is.factor)) %>% names()  
```

En realidad para seleccionar a las variables no-numéricas habría que hacer:

```{r, eval = FALSE}
df %>% select_if(!(is.numeric(.))) #- NO funciona
df %>% select_if(~ !is.numeric(.)) %>% names()     #- anonymous function but select_if
df %>% select(where(~ !is.numeric(.))) %>% names() #- anonymous functions & where
df %>% select(where(purrr::negate(is.numeric)))  %>% names()   #- purrr::negate()!!!!
```


Para visualizar rápidamente los valores de las variables categóricas, tenemos `inspectdf::inspect_cat()`, que nos devuelve un gráfico con la distribución de todas las variables categóricas.



```{r}
inspectdf::inspect_cat(df) %>% inspectdf::show_plot(high_cardinality = 1)
```

<br>

- También podemos hacerlo con `DataExplorer::plot_bar()`:


```{r}
DataExplorer::plot_bar(df)
```

```{r}
DataExplorer::plot_bar(df, by = "sexo_bebe.f")
```

<br>

- O con el paquete `SmartEDA`


```{r, eval = FALSE}
SmartEDA::ExpCatViz(df, Page = c(3,3))
```



<br>

----------------

<br>


## 3.8 Outliers

Un *outlier* es una observación que difieres, que está alejada, del resto de observaciones. ¿Qué hacer con los *outliers*? pues como todo depende ... [Aquí](https://statsandr.com/blog/outliers-detection-in-r/) y [aquí](http://r-statistics.co/Outlier-Treatment-With-R.html) tienes dos buenos posts sobre el tema.

Una forma rápida de identificar los outliers es usar `performance::check_outliers()` o `dlookr::diagnose_outlier()`. Para imputar un valor a los *outliers* se puede utilizar `dlookr::imputate_outlier()`



<br>

----------------

<br>


## 3.9 Paquetes con shiny


### a) `burro`

El paquete [`burro`](https://github.com/laderast/burro):

> burro attempts to make EDA accessible to a larger audience by exposing datasets as a simple **Shiny App** 


```{r, eval = FALSE}
#- devtools::install_github("laderast/burro")
df_small <- df %>% slice(1:1000)
burro::explore_data(df_small, outcome_var = colnames(df))
```

<br>

### b) `explore`

El paquete [`explore`](https://github.com/rolkra/explore).

> Instead of learning a lot of R syntax before you can explore data, the explore package enables you to have instant success. You can start with just one function - explore() - and learn other R syntax later step by step


- Podemos crear un shiny para explorar nuestro df completo:

```{r, eval = FALSE}
explore::explore(df)
```

- o solo explorar una o varias variables.

```{r, eval = FALSE}
df %>% explore::explore(parto_cesarea.f)
df %>% explore::explore(edad_madre.1, estudios_madre.ff, target = parto_cesarea.f)
```

<br>

### c) `ExPanDaR`

El paquete[ExPanDaR](https://joachim-gassen.github.io/ExPanDaR/) también permite explorar los datos a través de un shiny. Explican su funcionamiento en [este post](https://joachim-gassen.github.io/2019/12/explore-your-data-with-expand/) y [este otro](https://joachim-gassen.github.io/2019/04/customize-your-interactive-eda-explore-the-fuel-economy-of-the-u.s.-car-market/).

```{r, eval = FALSE}
library(ExPanDaR) #- remotes::install_github("joachim-gassen/ExPanDaR")

ExPanD(mtcars)
```


<br>

--------------

<br>


# 4. Tablas 

Muchas veces hay que presentar resultados básicos como una tabla de casos o de frecuencias. para ello tenemos muchas posibilidades:

## 4.1 Tablas con `summarytools`

El paquete [`summarytools`](https://github.com/dcomtois/summarytools). 

> `summarytools` is an R package providing tools to neatly and quickly summarize data. It can also make R a little easier to learn and to use, especially for data cleaning and preliminary analysis. 


- La función `freq()` provee tablas con conteos y frecuencias


```{r, results = "asis"}
summarytools::freq(df$sexo_bebe.f, style = "rmarkdown")
```

<br>

- tabulación cruzada entre dos variables categóricas:

```{r, results = "asis"}
summarytools::ctable(df$sexo_bebe.f, df$parto_cesarea.f)
```

<br>

- Incluso se pueden hacer test chi-cuadrado


```{r, results = "asis"}
summarytools::ctable(df$estudios_madre.ff, df$parto_normal.f, chisq = TRUE)
```

--------

<br>


## 4.2 Tablas con `janitor`


La verdad es que [`janitor`](http://sfirke.github.io/janitor/) es un paquete fantástico!!! 

> janitor has simple functions for examining and cleaning dirty data. It was built with beginning and intermediate R users in mind and is optimized for user-friendliness. Advanced R users can already do everything covered here, but with janitor they can do it faster and save their thinking for the fun stuff.



-  hacer (y dar formato) a tablas de 1, 2 o 3 variables:

```{r}
df %>% janitor::tabyl(parto_cesarea.f) %>% gt::gt()
```


```{r}
df %>% janitor::tabyl(parto_cesarea.f, estudios_madre.ff) %>% gt::gt()
```

<br>

```{r}
df %>% janitor::tabyl(parto_cesarea.f, estudios_madre.ff, sexo_bebe.f)  #- xq no podemos usar gt::gt()
```

-----------

<br>

## 4.3 Tablas con R-base

Las tablas con R-base tampoco están mal. El problema que tienen es que los resultados no se almacenan en data.frames, si no en matrices.

```{r}
table(df$parto_cesarea.f, df$estudios_madre.ff, useNA = "always") 
```

- Como se almacenan en matrices, para poder graficarlas con `gt` primero las hemos de convertir a data.frame y casi seguro que habrá que hacer uso de `pivot_wider()`

```{r}
my_tabla <- table(df$parto_cesarea.f, df$estudios_madre.ff) 
my_tabla %>% as.data.frame() %>% 
            pivot_wider(names_from = 2, values_from = 3) %>% 
            gt::gt()
```

<br>

- `mosaicplot()`, un gráfico que me gusta del sistema de R-base

```{r}
zz <- prop.table(my_tabla, 1)
zz <- zz[1:2,]   #- tengo que hacer esto xq hay NA's en la tabla
mosaicplot(t(zz), color = TRUE, main = "% de Cesáreas para niveles educativos de la madre")
```


-----------------

<br>


# 5. Contrastes 


Evidentemente R permite implementar múltiples técnicas y modelos estadísticos, así como hacer múltiples y variados contrastes de hipótesis. En [esta pagina web](https://www.statmethods.net/stats/index.html) tenéis un buena introducción a estos tema. Veamos algún ejemplo:


- `t-test`: <https://statistics.berkeley.edu/computing/r-t-tests>

```{r, eval = TRUE}
t.test(df$peso_bebe, mu = 3250)
t.test(df$peso_bebe ~ df$parto_cesarea.f)
```


- correlación entre dos variables cuantitativas

```{r, eval = FALSE}
library(Hmisc)
Hmisc::rcorr(as.matrix(df_numeric)) 
```



- un ejemplo para ver si hay diferencias en el peso en función de los estudios de la madre (!!!!)

```{r}
library(purrr)
library(broom)
df %>% group_by(estudios_madre.ff) %>% 
  summarise(t_test = list(t.test(peso_bebe))) %>% 
  mutate(tidied = map(t_test, tidy)) %>% 
  tidyr::unnest(tidied) %>% 
  ggplot(aes(estimate, estudios_madre.ff)) + geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) + 
  labs(title = "Peso del bebe para diferentes niveles educativos de la madre")
```




- `chi-squared test`: <https://data-flair.training/blogs/chi-square-test-in-r/>


```{r}
chisq.test(df$parto_cesarea.f, df$sexo_bebe.f)
chisq.test(df$parto_cesarea.f, df$estudios_madre.ff) 
```



- [Aquí](https://mgimond.github.io/Stats-in-R/index.html) puedes encontrar un curso sobre como implementar los contrastes estadísticos más habituales con R.


En [este post](https://lindeloev.github.io/tests-as-linear/), Jonas Kristoffer Lindeløv, nos presenta un cuadro con los contrastes de hipótesis más frecuentes y cómo efectuar esos contrastes en el contexto de los modelos de regresión con R.


<br>

------------------


# 6. Modelos


En este apartado vamos a ver (un poco^[La verdad es que con la cantidad de materiales fantásticos que hay sobre R, no hay necesidad de explicar o escribir sobre todos los temas]) como estimar modelos lineales y no lineales con R. Para una introducción a la estimación de modelos en R puedes ir [aquí](https://m-clark.github.io/R-models/#introduction).


Además, tenemos/tengo suerte de que me puedo apoyar en lo que ya habéis visto en Econometría y otras asignaturas. El objetivo, aparte de ver como se pueden hacer análisis de regresión con R, es ir preparando el camino para introducir algo de Machine Learning.

Antes restrinjamos aún un poco más el df con los datos sobre bebes:


```{r}
df_m <- df %>% select(peso_bebe, parto_nn_semanas, sexo_bebe.f, parto_cesarea.f, edad_madre.1, edad_padre.1, estudios_madre.ff, estudios_padre.ff)
df_m <- df_m %>% drop_na()
```



## 6.1 Modelos lineales

La función para estimar modelos lineales es `lm()`, así que vamos a utilizarla para estimar nuestro primer modelo con R. Estimaremos un modelo lineal con variable a explicar el peso del bebe (`peso_bebe`) en función de todas las demás variables en `df_m`.



```{r, eval = TRUE}
mod_1 <- lm(peso_bebe ~ . , data = df_m)
summary(mod_1)
```


Generalmente las **variables categóricas** se introducen en los modelos mediante **variables dummies**. Crear dummies en R es sencillo, solo tienes que tener los datos como factor o como texto y R creará las dummies por ti cuando introduzcas la variable en `lm()`. Eso sí, es más fácil elegir la categoría de referencia si la variable es un factor.

Para entender como fija los regresores para las variables categóricas, mira esto:
 
```{r}
levels(df$sexo_bebe.f)
levels(df$estudios_madre.ff)
```
 
Si necesitas cambiar la categoría de referencia siempre puedes usar `forcast::fct_relevel()`

```{r}
zz <- forcats::fct_relevel(df$sexo_bebe.f, "bebito")
levels(zz)
```


Veamos que hay en el objeto `mod_1`


```{r, eval = FALSE}
str(mod_1)
#listviewer::jsonedit(mod_1, mode = "view") ## Interactive option
```


#### Especificación del modelo

Lógicamente, a veces querremos seleccionar las variables explicativas:

```{r, eval = TRUE}
mod_1 <- lm(peso_bebe ~ parto_nn_semanas, data = df_m)
mod_1 <- lm(peso_bebe ~ parto_nn_semanas + sexo_bebe.f + parto_cesarea.f + edad_madre.1 , data = df_m)
mod_1 <- lm(log(peso_bebe) ~ log(parto_nn_semanas), data = df_m)

summary(mod_1)
```


Si queremos introducir interacciones entre los regresores, podemos usar el operador `:`, aunque casi mejor hacerlo directamente con `I()`:

```{r}
mod_1 <- lm(peso_bebe ~ parto_nn_semanas + parto_nn_semanas:edad_madre.1, data = df_m)
mod_1 <- lm(peso_bebe ~ parto_nn_semanas + I(parto_nn_semanas*edad_madre.1), data = df_m)

summary(mod_1)
```




Si queremos introducir las variables originales y también las interacciones entre ellas, podemos hacerlo directamente o o utilizar el operador `*`:


```{r}
mod_1 <- lm(peso_bebe ~ parto_nn_semanas + edad_madre.1 + parto_nn_semanas:edad_madre.1, data = df_m)
mod_1 <- lm(peso_bebe ~ parto_nn_semanas + edad_madre.1 + I(parto_nn_semanas*edad_madre.1), data = df_m)
mod_1 <- lm(peso_bebe ~ parto_nn_semanas*edad_madre.1, data = df_m)
mod_1 <- lm(peso_bebe ~ parto_nn_semanas*estudios_madre.ff, data = df_m)

summary(mod_1)
```

Recuerda que si queremos introducir algún regresor que sea la multiplicación de dos variables, tendremos que hacerlo con `I()`


```{r}
mod_1 <- lm(peso_bebe ~ parto_nn_semanas + I(parto_nn_semanas*parto_nn_semanas), data = df_m)

summary(mod_1)
```


También puede sernos de utilidad la función `poly()`

```{r}
mod_1 <- lm(peso_bebe ~ poly(parto_nn_semanas, degree = 3), data = df_m)

summary(mod_1)
```

<br>

#### Resultados de estimación


Una vez que sabemos la sintaxis de `stats::lm()`, vamos a ver como podemos acceder a la información que devuelve `lm()`. Ya hemos visto que los resultados se almacenan en una lista, así que podemos utilizar los operadores habituales de las listas para acceder a la información. Por ejemplo:


```{r, eval = FALSE}
zz_betas     <- mod_1[[1]]

zz_residuals <- mod_1[[2]]
zz_residuals <- mod_1[["residuals"]]
zz_residuals <- mod_1$residuals

zz_betas_1   <- mod_1[[1]]      #- [[ ]] doble corchete
zz_betas_2 <- mod_1[1]          #- []    corchete simple

zz_betas_1a <- mod_1[["coefficients"]]   #- doble corchete 
zz_betas_1c <- mod_1$coefficients        #- $

zz_betas_2a <- mod_1["coefficients"]     #- single corchete
```

Afortunadamente ya hay construidas algunas funciones para manipular/ver los resultados de estimación. Por ejemplo:



```{r, eval = FALSE}
mod_1 <- lm(peso_bebe ~ parto_nn_semanas + edad_madre.1 + sexo_bebe.f , data = df_m)

summary(mod_1)                   #- tabla resumen
summary(mod_1)$coefficients      #- tabla resumen con los coeficientes
coefficients(mod_1)              #- coeficientes estimados
confint(mod_1, level = 0.95)     #- Intervalos de confianza para los coeficientes
#fitted(mod_1)                   #- predicciones (y-sombrero, y-hat)
#residuals(mod_1)                #- vector de residuos
#model.matrix(mod_1)             #- extract the model matrix
anova(mod_1)                     #- ANOVA
vcov(mod_1)                      #- matriz de var-cov para los coeficientes 
#influence(mod_1)                #- regression diagnostics 
# diagnostic plots
layout(matrix(c(1, 2, 3, 4), 2, 2))   #- optional 4 graphs/page
plot(mod_1)                      #-
library(ggfortify)
autoplot(mod_1, which = 1:6, ncol = 2, colour = "steelblue")
```

<br>


#### Más utilidades

Hay muchas más funciones para valorar la idoneidad de un modelo. Por ejemplo [aquí](https://www.statmethods.net/stats/rdiagnostics.html).

<br>

El paquete [`GGally`](https://ggobi.github.io/ggally/) permite hacer muchos análisis, por ejemplo:

- gráfico de los coeficientes estimados


```{r}
GGally::ggcoef(mod_1)
```

- y muchas más cosas

```{r}
df_jugar <- df_m %>% select(edad_madre.1, sexo_bebe.f, parto_nn_semanas)
GGally::ggpairs(df_jugar)
```

<br>


#### Predicciones

Para hacer **predicciones** para observaciones fuera de la muestra has de usar la función `predict()`. Has de proporcionar las observaciones a predecir como un df.

```{r, eval = FALSE}
nuevas_observaciones <- df_m %>% slice(c(3, 44, 444))

predict(mod_1, newdata = nuevas_observaciones)  #- predicciones puntuales
predict(mod_1, newdata = nuevas_observaciones, type = 'response', se.fit = TRUE)  #- tb errores estándar  predictions
predict(mod_1, newdata = nuevas_observaciones, interval = "confidence")  #- intervalo (para el valor esperado)
predict(mod_1, newdata = nuevas_observaciones, interval = "prediction")  #- intervalo (para valores individuales)
```

<br>

#### Errores estándar robustos

Lo habitual es que al estimar un modelo, tengamos situaciones de heterocedasticidad, clustering etc ... Podemos ajustar los errores estándar a estas situaciones. 

El paquete de referencia para estos temas solía ser [`sandwich`](https://cran.r-project.org/web/packages/sandwich/index.html), aunque quizás ya haya ocupado su lugar el paquete [`estimatr`](https://declaredesign.org/r/estimatr/). 

Por ejemplo se pueden obtener errores estándar robustos con `estimatr::lm_robust()`^[Por defecto, el paquete usa Eicker-Huber-White robust standard errors, habitualmente conocidos como errores estándar “HC2”. Se pueden especificar otros métodos con la opción `se_type`. Por ejemplo se puede utilizar el método usado por defecto en Stata. [Aquí](https://declaredesign.org/r/estimatr/articles/stata-wls-hat.html) puedes encontrar porque los errores estándar de Stata difieren de los usados en R y Phyton].  



```{r, eval = FALSE}
#- install.packages("emmeans")
mod_1 <- lm(peso_bebe ~ parto_nn_semanas, data = df_m)
mod_1_ee <- estimatr::lm_robust(peso_bebe ~ parto_nn_semanas, data = df_m)
```

<br>


#### Paquete `broom`


Un opción interesante es utilizar el paquete [`broom`](https://broom.tidyverse.org/). Este paquete tiene 3 funciones útiles:

  - `tidy()`
  
  - `augment()`
  
  - `glance()`


```{r}
zz <- broom::tidy(mod_1, conf.int = TRUE)
zz %>% pjpv2020.01::pjp_f_decimales(nn = 2)  %>% gt::gt()
```


```{r}
mod_1 %>% broom::glance() %>% select(adj.r.squared, p.value)
broom::glance(mod_1)
```


```{r}
zz <- broom::augment(mod_1)
```



- un ejemplo sencillo en el que se ve la utilidad de `broom`

```{r}
mod_1 %>% broom::tidy() %>% filter(p.value < 0.05)
```


- Un ejemplo de uso de `broom`, pero antes vamos a recordar alguna cosa de `ggplot2`:


```{r, eval = FALSE}
ggplot(data = df_m, mapping = aes(x = edad_madre.1, y = peso_bebe,  color = estudios_madre.ff)) +
      geom_point(alpha = 0.1) +  geom_smooth(method = "lm")

ggplot(data = df_m, mapping = aes(x = edad_madre.1, y = peso_bebe,  color = sexo_bebe.f)) +
      geom_point(alpha = 0.1) +  geom_smooth(method = "lm")

ggplot(data = df_m, mapping = aes(x = edad_madre.1, y = peso_bebe,  color = sexo_bebe.f)) +
      geom_point(alpha = 0.1) +  geom_smooth()
```


Ahora sí viene el ejemplo en que se usa el paquete `broom`


```{r}
mod_1 <- lm(peso_bebe ~ edad_madre.1 + sexo_bebe.f , data = df_m)
summary(mod_1)

td_mod_1 <- mod_1 %>% broom::augment(data = df_m) 

td_mod_1 %>% ggplot(mapping = aes(x = edad_madre.1, y = peso_bebe, color = sexo_bebe.f)) +
                geom_point(alpha = 0.1) +
                geom_line(aes(y = .fitted, group = sexo_bebe.f))
```


```{r}
td_mod_1 %>% ggplot(mapping = aes(x = edad_madre.1, y = .fitted,  color = sexo_bebe.f)) +
                geom_line(aes(group = sexo_bebe.f)) 
```

(!!!!) Por ejemplo también permite fácilmente estimar modelos por grupos:
 
```{r}
library(broom)
df_m %>% group_by(sexo_bebe.f) %>% do(tidy(lm(peso_bebe ~ parto_nn_semanas, .)))
```
 

O hacer gráficos de los intervalos de los coeficientes:

```{r}
td_mod_1 <- tidy(mod_1, conf.int = TRUE)
ggplot(td_mod_1, aes(estimate, term, color = term)) +
    geom_point() +
    geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
    geom_vline(xintercept = 0)
```
 
 o este:
 
 
```{r}
mod_1 %>% augment(data = df_m) %>%
 ggplot(mapping = aes(x = parto_nn_semanas , y = .fitted, color = sexo_bebe.f)) +
   geom_point(mapping = aes(y = peso_bebe), alpha = 0.1) +
   geom_line()
```

 o este:
 
```{r}
mod_1 %>% augment(data = df_m) %>%
 ggplot(mapping = aes(x = parto_nn_semanas , y = .fitted, color = sexo_bebe.f)) +
   geom_point(mapping = aes(y = peso_bebe), alpha = 0.1) +
   geom_line() +
  facet_wrap(vars(estudios_madre.ff))
```
 
 
 <br>

 
 
### Comparación de modelos

```{r}
mod_1 <- lm(peso_bebe ~ parto_nn_semanas + edad_madre.1,  data = df_m)
mod_2 <- lm(peso_bebe ~ parto_nn_semanas + edad_madre.1 + I(parto_nn_semanas*edad_madre.1), data = df_m)

anova(mod_1, mod_2)
```


```{r}
AIC(mod_1, mod_2)
```


```{r}
lmtest::lrtest(mod_1, mod_2)    
```

(!!!)  Vamos a ordenar los modelos en función de su AIC. Para ello vamos a crear una lista con modelos

```{r}
modelos <- list(mod_1 <- lm(peso_bebe ~ parto_nn_semanas + edad_madre.1,  data = df_m),
                mod_2 <- lm(peso_bebe ~ parto_nn_semanas + edad_madre.1 + I(parto_nn_semanas*edad_madre.1), data = df_m)  )

modelos_ordered_AIC <- purrr::map_df(modelos, broom::glance, .id = "model") %>% arrange(AIC)

modelos_ordered_AIC %>% gt::gt()
```

- una tabla con `broom` (antes creamos una función (!!!!)):


```{r}
my_kable <- function(df){ gt::gt(mutate_if(df, is.numeric, round, 2)) }

tidy(mod_1) %>% my_kable
```


<br>


### El paquete `modelr`

Con el paquete [`modelr`](https://modelr.tidyverse.org/index.html) podemos *fácilmente* comparar las predicciones de varios modelos: 


```{r}
zz <- df_m %>% modelr::gather_predictions(mod_1, mod_2)
```

Para ver mejor las predicciones de los 2 modelos habrá que pasar zz a formato ancho:

```{r}
zz1 <- pivot_wider(zz, names_from = model, values_from = pred)
```

-------------

<br>


## 6.2 Modelos GLM

Por ejemplo un Logit:


```{r}
mod_logit <- glm(parto_cesarea.f ~ peso_bebe + parto_nn_semanas + edad_madre.1 + estudios_madre.ff, family = binomial(link = "logit"), data = df_m)

summary(mod_logit)
```


<br>

En los modelos lineales, calcular efectos marginales es sencillo, pero el Logit es un modelo no lineal. Para calcular efectos marginales en modelos no lineales podemos usar el paquete [`margins`](https://github.com/leeper/margins). Por ejemplo, calculemos el Efecto marginal medio o average marginal effect (AME) en `mod_logit`.


```{r}
mod_1_AME <- mod_logit %>% margins::margins() %>% summary() 
mod_1_AME
```

Si queremos calcular efectos marginales para unos valores concretos de los regresores

```{r, eval = FALSE}
mod_logit %>% margins::margins(at = list(parto_nn_semanas = c(25, 35), estudios_madre.ff = c("Primarios", "Medios", "Universidad")),  variables = "edad_madre.1" ) 
```


Si prefieres visualizarlo, utiliza `margins::cplot()`:


```{r, eval = FALSE}
margins::cplot(mod_logit, x = "estudios_madre.ff", dx = "edad_madre.1", what = "effect", drop = TRUE)
margins::cplot(mod_logit, x = "estudios_madre.ff", dx = "edad_madre.1")
```

### El pkg `ggeffects`

El paquete [`ggeffects`](https://strengejacke.github.io/ggeffects/) proporciona otra forma de calcular efectos marginales en modelos de regresión:

> Results of regression models are typically presented as tables that are easy to understand. For more complex models that include interaction or quadratic / spline terms, tables with numbers are less helpful and difficult to interpret. In such cases, marginal effects are far easier to understand. In particular, the visualization of marginal effects allows to intuitively get the idea of how predictors and outcome are associated, even for complex models.


No lo he probado aún, pero se puede usar en una gran variedad de modelos.
Estimated Marginal Means and Marginal Effects from Regression Models



<br>

## 6.3 Tablas para modelos



### Con `sjPLot` y friends ...



```{r, eval = FALSE}
m1 <- lm(peso_bebe ~ parto_nn_semanas + sexo_bebe.f + edad_madre.1 + edad_padre.1, data = df)
m2 <- lm(peso_bebe ~ parto_nn_semanas + sexo_bebe.f + edad_madre.1 + edad_padre.1. + estudios_madre.ff + estudios_padre.ff, data = df)
sjPlot::tab_model(m1)
sjPlot::plot_model(m1, sort.est = TRUE)
sjPlot::tab_model(m1, m2)
```


<br>

### Con `stargazer`

```{r, results = "asis"}
mod_1 <- lm(peso_bebe ~ parto_nn_semanas + sexo_bebe.f , data = df_m)
stargazer::stargazer(mod_1, type = "html")
```


```{r, eval = FALSE, echo = FALSE}
mod_1 <- lm(peso_bebe ~ parto_nn_semanas + sexo_bebe.f , data = df_m)
stargazer::stargazer(mod_1, type = "text")
```

<br>


### Con `modelsummary`


```{r}
#remotes::install_github('vincentarelbundock/modelsummary')
library(modelsummary)

mys_modelitos <- list()
mys_modelitos[["peso_bebe:  OLS 1"]]    <-  lm(peso_bebe    ~ parto_nn_semanas + sexo_bebe.f,            df_m)
mys_modelitos[["peso_bebe:  OLS 2"]]    <-  lm(peso_bebe    ~ parto_nn_semanas + sexo_bebe.f + edad_madre.1 , df_m)
mys_modelitos[["CESAREA: Logit 1"]]  <- glm(parto_cesarea.f ~ parto_nn_semanas + sexo_bebe.f , data = df_m, family = binomial(link = "logit"))

mm <- msummary(mys_modelitos, title = "Resultados de estimación")
mm
```

<br>

### Con [`reports`](https://github.com/trinker/reports)

- informes con el paquete `reports`

```{r, results = "asis", eval = FALSE}
library(report) #- devtools::install_github("neuropsychology/report")
my_model <- lm(peso_bebe ~ parto_nn_semanas + sexo_bebe.f, df_m)
rr <- report(my_model, target = 1)
rr
```

<br>

```{r, eval = FALSE}
report::as.report(rr)
```

<br>

Recuerda también que se puede obtener la ecuación (en latex) de un modelo con:
 
 
```{r, eval = FALSE}
library(equatiomatic)  #- remotes::install_github("datalorax/equatiomatic")
extract_eq(mod_1)
extract_eq(mod_1, use_coefs = TRUE)
```

<br>


## 6.4 Otros modelos/técnicas

En realidad sólo voy a insistir en que con R se pueden implementar una gran variedad de modelos y técnicas estadísticas. Puedes ver algunos ejemplos en [este libro](https://m-clark.github.io/R-models/#introduction).
Hay materiales excelentes, tanto libros, como tutoriales, posts, etc ...  que puedes encontrar fácilmente en internet.


<br>

<blockquote class="twitter-tweet" data-dnt="true" data-theme="dark"><p lang="en" dir="ltr">My <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> learning path:<br><br>1. Install R<br>2. Install RStudio<br>3. Google &quot;How do I [THING I WANT TO DO] in R?&quot;<br><br>Repeat step 3 ad infinitum.</p>&mdash; Jesse Mostipak (@kierisi) <a href="https://twitter.com/kierisi/status/898534740051062785?ref_src=twsrc%5Etfw">August 18, 2017</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 

<br>

 
<br>

------------------------

<br>

## 6.5 Machine Learning

Simplemente señalar que es un área de estudio enorme y en constante evolución. En R hay varios entornos/enfoques para hacer ML. Creo que el que más futuro tiene es [`tidymodels`](https://www.tidymodels.org/). Un [post introductorio](https://meghan.rbind.io/post/tidymodels-intro/) sobre tidymodels. [Un ejemplo](https://www.tidymodels.org/start/case-study/) de uso de tidymodels. [Otro ejemplo](https://scotinastats.rbind.io/2020/07/30/hotel-bookings-tidymodels/).

Simplemente algunas referencias:

- [Machine Learning for Everyone](https://vas3k.com/blog/machine_learning/). Un post introductorio.

- [Hands-On Machine Learning with R](https://bradleyboehmke.github.io/HOML/DT.html). Un buen bookdown

- [101 Machine Learning Algorithms for Data Science with Cheat Sheets](https://blog.datasciencedojo.com/machine-learning-algorithms/). Cheatsheet de algoritmos ML.

- [101 Data Science Interview Questions, Answers, and Key Concepts](https://blog.datasciencedojo.com/data-science-interview-questions/). Post con cosas que **"deberías"** saber si quieres un trabajo como Data Scientist.

- [Machine learning essentials](https://biodatascience.github.io/statcomp/ml/essentials.html). Un tema de un curso sobre ML.

- [Introduction to Machine Learning with R](http://www.mpia.de/homes/dgoulier/MLClasses/Course%20-%20Introduction%20to%20Machine%20Learning%20for%20Scientists%20with%20R.html#chapter_2:_performance_measures). Otro curso sobre ML.

- [Machine Learning](https://m-clark.github.io/introduction-to-machine-learning/concepts.html). Bookdown centrado en explicar las principales ideas/conceptos de ML.


- [Machine Learning con R y caret](https://www.cienciadedatos.net/documentos/41_machine_learning_con_r_y_caret)por Joaquín Amat Rodrigo. A pesar de que el paquete `caret` ha sido sustituido por `tidymodels`, continua siendo una buena entrada a ML con R y en castellano, por Joaquín Amat Rodrigo.  

- [A Gentle Introduction to tidymodels](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/). 


- [Choosing the right estimator](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html). Guía  de `scikit-learn` para selección de modelos.

- [Una introducción visual al machine learning - I](http://www.r2d3.us/una-introduccion-visual-al-machine-learning-1/). Post introductorio pero muy bonito visualmente sobre ML. [Aquí](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/) la segunda parte.


- [R: MLR, Decision Trees and Random Forest to Predict MPG for 2019 Vehicles](https://blog.alpha-analysis.com/2019/06/predicting-mpg-for-2019-vehicles-using-r.html). Después lo implementan en [TensorFlow](https://blog.alpha-analysis.com/2019/08/r-tensorflow-multiple-linear-regression.html)

<br><br><br>



